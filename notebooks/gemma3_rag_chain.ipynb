{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6665db6a-b8aa-411b-936b-2709b1c29da1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:36:18.294483Z",
     "iopub.status.busy": "2025-05-24T05:36:18.293682Z",
     "iopub.status.idle": "2025-05-24T05:36:18.302164Z",
     "shell.execute_reply": "2025-05-24T05:36:18.300322Z",
     "shell.execute_reply.started": "2025-05-24T05:36:18.294407Z"
    },
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Setup and load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c38c36e-b5af-495a-922e-16a08a557075",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:18:50.209042Z",
     "iopub.status.busy": "2025-05-24T05:18:50.208635Z",
     "iopub.status.idle": "2025-05-24T05:18:51.095285Z",
     "shell.execute_reply": "2025-05-24T05:18:51.094458Z",
     "shell.execute_reply.started": "2025-05-24T05:18:50.209006Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "base_url=\"http://host.docker.internal:11434\"\n",
    "model_name = \"gemma3:27b\"\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=model_name,\n",
    "    base_url=base_url,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03de39d7-9087-44c6-bf14-6b99aaf8618a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:18:51.097380Z",
     "iopub.status.busy": "2025-05-24T05:18:51.096391Z",
     "iopub.status.idle": "2025-05-24T05:18:51.143787Z",
     "shell.execute_reply": "2025-05-24T05:18:51.143023Z",
     "shell.execute_reply.started": "2025-05-24T05:18:51.097343Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"mxbai-embed-large\",\n",
    "    base_url=base_url,\n",
    ")\n",
    "\n",
    "\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db3c5367-4401-4335-80f6-cfbe3ef7d1b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:18:57.278274Z",
     "iopub.status.busy": "2025-05-24T05:18:57.277331Z",
     "iopub.status.idle": "2025-05-24T05:19:01.372304Z",
     "shell.execute_reply": "2025-05-24T05:19:01.371522Z",
     "shell.execute_reply.started": "2025-05-24T05:18:57.278220Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) # openai file search chunck_size=800, chuck_overlap=400\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ba0352-c70e-4698-934e-7b1126d35f51",
   "metadata": {},
   "source": [
    "# Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "129104e5-08a4-4b8d-b60c-f4bccdac4fe4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:30:41.394747Z",
     "iopub.status.busy": "2025-05-24T05:30:41.393932Z",
     "iopub.status.idle": "2025-05-24T05:30:41.408394Z",
     "shell.execute_reply": "2025-05-24T05:30:41.407165Z",
     "shell.execute_reply.started": "2025-05-24T05:30:41.394679Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Query(BaseModel):\n",
    "    \"\"\"Optimized query for information retrieval\"\"\"\n",
    "    original_question: str = Field(description=\"user's original question\")\n",
    "    optimized_query: str = Field(description=\"optimized query for embedding\")\n",
    "\n",
    "\n",
    "structured_llm = llm.with_structured_output(Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97a8e28e-a314-4cc6-a7aa-1ceec18959cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:33:30.050254Z",
     "iopub.status.busy": "2025-05-24T05:33:30.048896Z",
     "iopub.status.idle": "2025-05-24T05:33:30.064741Z",
     "shell.execute_reply": "2025-05-24T05:33:30.063495Z",
     "shell.execute_reply.started": "2025-05-24T05:33:30.050177Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to complex and special queries.\"\"\"\n",
    "    print(\"=\"*40)\n",
    "    print(\"retrieve\")\n",
    "    print(query)\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    print(serialized)\n",
    "    print(\"End of retrieve\")\n",
    "    print(\"=\"*40)\n",
    "    return serialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2970984-a81a-499b-8009-c5b77394814f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:37:12.815078Z",
     "iopub.status.busy": "2025-05-24T05:37:12.814231Z",
     "iopub.status.idle": "2025-05-24T05:37:12.820448Z",
     "shell.execute_reply": "2025-05-24T05:37:12.819627Z",
     "shell.execute_reply.started": "2025-05-24T05:37:12.815034Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "Use the context below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer only using the context. If the context does not contain enough information, say you don't know.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "\n",
    "# You are an assistant for question-answering tasks.\n",
    "# Use the following pieces of retrieved context to answer\n",
    "# the question. If you don't know the answer, say that you\n",
    "# don't know. Use three sentences maximum and keep the\n",
    "# answer concise\n",
    "\n",
    "# Context:\n",
    "# {context}\n",
    "\n",
    "# Question:\n",
    "# {question}\n",
    "\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04dce876-e1dd-4b10-b514-756e58d63483",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:37:12.959631Z",
     "iopub.status.busy": "2025-05-24T05:37:12.959358Z",
     "iopub.status.idle": "2025-05-24T05:37:12.965384Z",
     "shell.execute_reply": "2025-05-24T05:37:12.964518Z",
     "shell.execute_reply.started": "2025-05-24T05:37:12.959608Z"
    }
   },
   "outputs": [],
   "source": [
    "# Wrap it properly to unpack the structured output:\n",
    "chain = (\n",
    "    structured_llm\n",
    "    | (lambda query: {\n",
    "        \"context\": retrieve(query.optimized_query),\n",
    "        \"question\": query.original_question\n",
    "      })\n",
    "    | prompt_template\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1cc412b4-1d6f-4afc-abe4-2096d35b5d10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:37:13.138735Z",
     "iopub.status.busy": "2025-05-24T05:37:13.137866Z",
     "iopub.status.idle": "2025-05-24T05:37:18.809365Z",
     "shell.execute_reply": "2025-05-24T05:37:18.807068Z",
     "shell.execute_reply.started": "2025-05-24T05:37:13.138663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "retrieve\n",
      "What is task decomposition?\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "End of retrieve\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Task decomposition is breaking down a complicated task into multiple steps. It can be done by an LLM with simple prompting (like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\"), by using task-specific instructions (e.g. \"Write a story outline.\"), or with human inputs. It’s also achieved through methods like Chain of Thought (CoT) and Tree of Thoughts, which explore reasoning possibilities step-by-step.\\n', additional_kwargs={}, response_metadata={'model': 'gemma3:27b', 'created_at': '2025-05-24T05:37:18.435044908Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3096222717, 'load_duration': 32482283, 'prompt_eval_count': 561, 'prompt_eval_duration': 452622052, 'eval_count': 98, 'eval_duration': 2610587915, 'model_name': 'gemma3:27b'}, id='run--c7bf6555-eb78-4531-8f4a-76214f51a7f0-0', usage_metadata={'input_tokens': 561, 'output_tokens': 98, 'total_tokens': 659})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Hi there how are you? What is task decomposition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ee45d9e-a859-40dd-8910-801d157c8414",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:37:18.815159Z",
     "iopub.status.busy": "2025-05-24T05:37:18.811984Z",
     "iopub.status.idle": "2025-05-24T05:37:22.327196Z",
     "shell.execute_reply": "2025-05-24T05:37:22.326528Z",
     "shell.execute_reply.started": "2025-05-24T05:37:18.815093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "retrieve\n",
      "When did the first man land on the moon?\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: GOALS:\n",
      "\n",
      "1. {{user-provided goal 1}}\n",
      "2. {{user-provided goal 2}}\n",
      "3. ...\n",
      "4. ...\n",
      "5. ...\n",
      "\n",
      "Constraints:\n",
      "1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n",
      "2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n",
      "3. No user assistance\n",
      "4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n",
      "5. Use subprocesses for commands that will not terminate within a few minutes\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\n",
      "\n",
      "The heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\n",
      "Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\n",
      "\n",
      "\n",
      "Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\n",
      "End of retrieve\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't know.\\n\", additional_kwargs={}, response_metadata={'model': 'gemma3:27b', 'created_at': '2025-05-24T05:37:21.960683911Z', 'done': True, 'done_reason': 'stop', 'total_duration': 609872536, 'load_duration': 45695989, 'prompt_eval_count': 460, 'prompt_eval_duration': 371242166, 'eval_count': 8, 'eval_duration': 192429105, 'model_name': 'gemma3:27b'}, id='run--1f07c650-cc64-43c9-8885-8c29975c813d-0', usage_metadata={'input_tokens': 460, 'output_tokens': 8, 'total_tokens': 468})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Hi there how are you? When the first man landed on the moon?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
